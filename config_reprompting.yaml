# Configuration for Reprompting Attacks
# This config focuses on reprompting-based adversarial attacks

global:
  netid: "bt4811"
  scratch_dir: "/u/bt4811"
  home_dir: "/u/bt4811/reasoning_attacks_res"
  project_dir: "/scratch/bt4811/adv_attacks"
  email: "bt4811@princeton.edu"
  no_slurm: false  # If true, generate Python commands instead of submitting SLURM jobs
  
  environment:
    conda_env: "/scratch/bt4811/adv_attacks/attenv"
    modules:
      - "anaconda3"
    
    huggingface:
      cache_dir: "/scratch/bt4811/huggingface"
      offline_mode: true
    
    wandb:
      entity: "bogdan-turbal-y"
      offline_mode: true
    
    openrouter:
      api_key: "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"
      base_url: "https://openrouter.ai/api/v1"
      model: "mistralai/mixtral-8x7b-instruct"
      

slurm_defaults:
  nodes: 1
  ntasks: 4
  mem: "128G"
  time: "06:00:00"
  gres: "gpu:4"
  constraint: "gpu80"
  mail_type: "ALL"

models:
  deepseek_r1:
    name: "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
    dtype: "bfloat16"
  
  attacker_mixtral:
    name: "mistralai/Mixtral-8x7B-Instruct-v0.1"
    dtype: "bfloat16"

datasets:
  cot150:
    input_csv: "reasoning_attacks/dataset/orthogonalized_outputs_cot150_2048.csv"

subexperiments:
  reprompting_base:
    description: "Reprompting attack with default settings"
    results_dir: "reprompting_base"
    model: "deepseek_r1"
    attacker_model: "attacker_mixtral"
    dataset: "cot150"
    
    experiment:
      num_iters: 15  # number of iterations for refinement
      num_branches: 8  # number of generated children from each reasoning string
      memory: 32  # buffer size for the search method
      K: 2  # bucket size for adding randomness to the feedbacks
      batch_size: 16
      max_examples: -1
      verbose: true
      num_gpus: 2 # Number of GPUs to use for parallel processing
      attacker_api: true  # If true, use OpenRouter API instead of local attacker model
      attacker_quantize: true  # Enable quantization for attacker model (ignored if attacker_api is true)
      attacker_quantize_bits: 4  # 4 or 8 bit quantization
      attacker_use_flash_attention: false  # Enable Flash Attention 2 for attacker model (ignored if attacker_api is true)
    
    parallel:
      num_jobs: 2
      range_type: "percentage"
      range_spec: "0.0:1.0"
    
    slurm:
      time: "06:00:00"
      gres: "gpu:8"
      nodes: 1
      ntasks: 8
      mem: "256G"
      constraint: null  # Explicitly remove constraint from defaults
    
    wandb_project: "reprompting_attacks"

